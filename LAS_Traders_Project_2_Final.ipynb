{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lukehartfield/Portfolio-Optimization/blob/main/LAS_Traders_Project_2_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pillow"
      ],
      "metadata": {
        "id": "3YmWrujNt_8-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29ae661d-6f14-4e54-d4c4-f2d4c0d4c2b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wrds"
      ],
      "metadata": {
        "id": "1-scoWmowY4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 584
        },
        "outputId": "20468e22-c649-4743-e288-bf4b88ab7723"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wrds\n",
            "  Downloading wrds-3.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting packaging<=24.2 (from wrds)\n",
            "  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: pandas<2.3,>=2.2 in /usr/local/lib/python3.12/dist-packages (from wrds) (2.2.2)\n",
            "Collecting psycopg2-binary<2.10,>=2.9 (from wrds)\n",
            "  Downloading psycopg2_binary-2.9.10-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: sqlalchemy<2.1,>=2 in /usr/local/lib/python3.12/dist-packages (from wrds) (2.0.43)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas<2.3,>=2.2->wrds) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<2.3,>=2.2->wrds) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<2.3,>=2.2->wrds) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<2.3,>=2.2->wrds) (2025.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy<2.1,>=2->wrds) (3.2.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy<2.1,>=2->wrds) (4.15.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<2.3,>=2.2->wrds) (1.17.0)\n",
            "Downloading wrds-3.4.0-py3-none-any.whl (14 kB)\n",
            "Downloading packaging-24.2-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading psycopg2_binary-2.9.10-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: psycopg2-binary, packaging, wrds\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 25.0\n",
            "    Uninstalling packaging-25.0:\n",
            "      Successfully uninstalled packaging-25.0\n",
            "Successfully installed packaging-24.2 psycopg2-binary-2.9.10 wrds-3.4.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "packaging"
                ]
              },
              "id": "65bd0ab625384304afe3dda2271f53bb"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install adjustText"
      ],
      "metadata": {
        "id": "WsjFdXHdz_RV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16f5aa0b-2d39-40e1-ca60-fe15de08f61d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting adjustText\n",
            "  Downloading adjustText-1.3.0-py3-none-any.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from adjustText) (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from adjustText) (3.10.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from adjustText) (1.16.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->adjustText) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->adjustText) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->adjustText) (4.60.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->adjustText) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->adjustText) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->adjustText) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->adjustText) (3.2.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->adjustText) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->adjustText) (1.17.0)\n",
            "Downloading adjustText-1.3.0-py3-none-any.whl (13 kB)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 179, in exc_logging_wrapper\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PU_g8qzk4Xg9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pandas.tseries.offsets import MonthEnd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from datetime import datetime\n",
        "from scipy.optimize import minimize\n",
        "import math\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "from datetime import date\n",
        "from calendar import monthrange\n",
        "\n",
        "import wrds\n",
        "db = wrds.Connection()\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# 1) Universe: all valid tickers (US common shares only)\n",
        "# -------------------------\n",
        "\n",
        "# NOTE: This query does not select a 'date' column, so don't pass date_cols=[\"date\"].\n",
        "q = \"\"\"\n",
        "    SELECT DISTINCT\n",
        "        mse.ticker\n",
        "    FROM crsp.msf AS msf\n",
        "    JOIN crsp.msenames AS mse\n",
        "      ON msf.permno = mse.permno\n",
        "     AND mse.namedt <= msf.date\n",
        "     AND (msf.date <= mse.nameendt OR mse.nameendt IS NULL)  -- handle open-ended names\n",
        "    WHERE mse.shrcd IN (10, 11)          -- U.S. common shares only (no ADRs)\n",
        "      AND mse.exchcd IN (1, 2, 3)        -- NYSE/AMEX/NASDAQ\n",
        "\"\"\"\n",
        "all_tickers = db.raw_sql(q)  # <-- no date_cols arg here\n",
        "all_ticker_set = set(all_tickers[\"ticker\"])\n",
        "\n",
        "# -------------------------\n",
        "# 2) Interactive ticker entry\n",
        "# -------------------------\n",
        "\n",
        "def get_tickers(all_ticker_set, prompt='Enter tickers separated by commas (e.g., AAPL, MSFT, TSLA) or quit to exit:'):\n",
        "  \"\"\"Accept commas as separators and returns clean tickers\n",
        "  (uppercase/lowercase, letters/numbers, no duplicates, exist in the universe set).\"\"\"\n",
        "\n",
        "  allow = re.compile(r\"^[A-Z0-9]+$\", re.IGNORECASE)   # alphanum only\n",
        "\n",
        "  while True:\n",
        "      raw = input(prompt)\n",
        "\n",
        "      # Step 1: split, strip spaces, force uppercase\n",
        "      tickers = [t.strip().upper() for t in raw.split(\",\") if t.strip()]\n",
        "\n",
        "      if raw == \"quit\":\n",
        "        print(\"See you next time.\")\n",
        "        return None\n",
        "\n",
        "      # Step 2: validate\n",
        "      not_found = [t for t in tickers if t not in all_ticker_set]\n",
        "\n",
        "      # nothing inputted\n",
        "      if not tickers:\n",
        "          print(\"You must enter at least one ticker.\")\n",
        "          continue\n",
        "      # tickers inputted but need to check if exist and correct\n",
        "      seen = set()\n",
        "      distinct_tickers = []\n",
        "      invalid = []\n",
        "\n",
        "      for t in tickers:\n",
        "        if not allow.match(t):\n",
        "          invalid.append(t)\n",
        "          continue\n",
        "        if t not in seen:\n",
        "          seen.add(t)\n",
        "          distinct_tickers.append(t)\n",
        "\n",
        "      if invalid:\n",
        "        print(\"Invalid ticker format (alphanumeric only): \" + \", \".join(invalid))\n",
        "        continue\n",
        "\n",
        "      if not_found:\n",
        "        print(f\"Ticker(s) not found in WRDS: {', '.join(not_found)}. Try again.\")\n",
        "        continue\n",
        "\n",
        "      return distinct_tickers\n",
        "\n",
        "# -------------------------\n",
        "# 3) Interactive portfolio weights\n",
        "# -------------------------\n",
        "\n",
        "# expects tickers to already be a list\n",
        "def get_portfolio_weights(tickers,\n",
        "                          prompt=\"What is your original portfolio weight for each ticker, in order, separated by commas? Enter 10 for 10%, equal for equal weights, or quit to exit\\n\",\n",
        "                          min_w=-100.0, max_w=100.0, tolerance=1e-4): # ensures within weight constraint and prevents rounding error\n",
        "  \"\"\"Accept percent inputs and returns weights in decimal form, matching the\n",
        "  number of tickers while also fitting the weight constraint.\"\"\"\n",
        "\n",
        "  # tickers = list(tickers)\n",
        "  n = len(tickers)\n",
        "  token_re = re.compile(r\"^[+-]?[0-9]*\\.?[0-9]+$\")  # allow negatives like -10 or -0.5\n",
        "\n",
        "  while True:\n",
        "    raw = input(prompt).strip()\n",
        "    user_w = [w for w in re.split(r\"[,\\s]+\", raw) if w]\n",
        "\n",
        "    if raw == \"quit\":\n",
        "      print(\"See you next time.\")\n",
        "      break\n",
        "\n",
        "    # need equal option\n",
        "    if raw.lower() == \"equal\":\n",
        "      eq = [1.0 / n] * n\n",
        "      return pd.Series(eq, index=tickers)\n",
        "\n",
        "    # n weights dont match n tickers\n",
        "    if len(user_w) != n:\n",
        "      print(f\"Please provide exactly {n} weights to match: {', '.join(tickers)}\")\n",
        "      continue\n",
        "\n",
        "    bad = [w for w in user_w if not token_re.match(w)]\n",
        "    if bad:\n",
        "      print(\"Invalid token(s). Use percent numbers only (e.g., 10, 1.2, -5). Offenders: \" + \", \".join(bad))\n",
        "      continue\n",
        "\n",
        "    if not raw:\n",
        "      print(\"Please enter your portfolio weights.\")\n",
        "\n",
        "    w_pct = [float(w) for w in user_w]\n",
        "\n",
        "    # per-asset bounds\n",
        "    below = [i for i, v in enumerate(w_pct) if v < min_w]\n",
        "    above = [i for i, v in enumerate(w_pct) if v > max_w]\n",
        "    if below or above:\n",
        "      if below:\n",
        "        print(\"Below minimum bound \"\n",
        "              f\"({min_w}%): \" + \", \".join(f\"{tickers[i]}={w_pct[i]}%\" for i in below))\n",
        "      if above:\n",
        "        print(\"Above maximum bound \"\n",
        "              f\"({max_w}%): \" + \", \".join(f\"{tickers[i]}={w_pct[i]}%\" for i in above))\n",
        "      continue\n",
        "\n",
        "    total = sum(w_pct)\n",
        "    if abs(total - 100.0) > tolerance:\n",
        "       print(f\"Total must be 100%. Your total is {total:.4f}%. Please adjust and try again.\")\n",
        "       continue\n",
        "\n",
        "    return pd.Series([v/100.0 for v in w_pct], index=tickers)\n",
        "\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# 4) Interactive date entry\n",
        "# -------------------------\n",
        "\n",
        "def get_dates():\n",
        "  \"\"\"Prompt for YYYY-MM, snap to month boundaries, validate order,\n",
        "  and ensure OOS starts after IS ends. Returns four YYYY-MM-DD strings.\"\"\"\n",
        "  ym_pat = re.compile(r\"^\\d{4}-(0[1-9]|1[0-2])$\")  # strict YYYY-MM\n",
        "\n",
        "  while True:\n",
        "    is_start_in = input(\"In-sample start (YYYY-MM): \").strip()\n",
        "    is_end_in   = input(\"In-sample end   (YYYY-MM): \").strip()\n",
        "    oos_start_in= input(\"Out-of-sample start (YYYY-MM): \").strip()\n",
        "    oos_end_in  = input(\"Out-of-sample end   (YYYY-MM): \").strip()\n",
        "\n",
        "    # exit, only one of these will show\n",
        "    if is_start_in.lower() in {\"q\", \"quit\"}: return print(\"See you next time.\")\n",
        "    if is_end_in.lower() in {\"q\", \"quit\"}: return print(\"See you next time.\")\n",
        "    if oos_start_in.lower() in {\"q\", \"quit\"}: return print(\"See you next time.\")\n",
        "    if oos_end_in.lower() in {\"q\", \"quit\"}: return print(\"See you next time.\")\n",
        "\n",
        "    # presence\n",
        "    if not is_start_in or not is_end_in or not oos_start_in or not oos_end_in:\n",
        "        print(\"All four dates are required in YYYY-MM. Try again.\")\n",
        "        continue\n",
        "\n",
        "    # format\n",
        "    if not (ym_pat.match(is_start_in) and ym_pat.match(is_end_in)\n",
        "            and ym_pat.match(oos_start_in) and ym_pat.match(oos_end_in)):\n",
        "        print(\"Incorrect format. Use YYYY-MM for each date. Try again.\")\n",
        "        continue\n",
        "\n",
        "    # parse and snap to boundaries\n",
        "    y,m = map(int, is_start_in.split(\"-\"));  is_start = date(y, m, 1)\n",
        "    y,m = map(int, is_end_in.split(\"-\"));    is_end   = date(y, m, monthrange(y, m)[1])\n",
        "    y,m = map(int, oos_start_in.split(\"-\")); oos_start= date(y, m, 1)\n",
        "    y,m = map(int, oos_end_in.split(\"-\"));   oos_end  = date(y, m, monthrange(y, m)[1])\n",
        "\n",
        "    # ordering checks: OOS dates need to be after IS dates\n",
        "    if is_start > is_end:\n",
        "        print(f\"In-sample start {is_start} must be <= end {is_end}. Try again.\")\n",
        "        continue\n",
        "    if oos_start > oos_end:\n",
        "        print(f\"Out-of-sample start {oos_start} must be <= end {oos_end}. Try again.\")\n",
        "        continue\n",
        "    if not (oos_start > is_end):\n",
        "        print(f\"OOS must start after IS ends. IS end {is_end}, OOS start {oos_start}. Try again.\")\n",
        "        continue\n",
        "\n",
        "# guard against out-of-reach OOS dates\n",
        "    try:\n",
        "      maxd = db.raw_sql(\"SELECT MAX(date) AS maxd FROM crsp.msf\")[\"maxd\"].iloc[0]  # noqa: F821\n",
        "      if pd.notna(maxd):\n",
        "        maxd = pd.to_datetime(maxd).date()\n",
        "        if oos_start > maxd:\n",
        "          print(f\"OOS start {oos_start} is after latest CRSP month {maxd}. Pick earlier dates.\"); continue\n",
        "        if oos_end > maxd:\n",
        "          print(f\"Clamping OOS end from {oos_end} to {maxd} (latest available).\")\n",
        "          oos_end = maxd\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    return tuple(d.strftime(\"%Y-%m-%d\") for d in (is_start, is_end, oos_start, oos_end))\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# 5) CRSP fetch helper\n",
        "# -------------------------\n",
        "\n",
        "# we felt a minimum observation of 24 seemed good because <2yrs is too noisy\n",
        "\n",
        "def fetch_crsp_returns(db, tickers, start_ymd, end_ymd, min_obs=24, permnos=None):\n",
        "    \"\"\" Pull monthly CRSP for tickers and date range, clean + combine delisting returns,\n",
        "    enforce coverage, and create a monthly return matrix (ret_total).\n",
        "    Returns ret_matrix, kept and dropped tickers.\"\"\"\n",
        "    if not tickers and not permnos:\n",
        "        raise ValueError(\"No tickers provided.\")\n",
        "\n",
        "    # CHANGE 2: build the ID filter once\n",
        "    if permnos is not None:\n",
        "      if len(permnos) == 0:\n",
        "            raise ValueError(\"permnos provided but empty.\")\n",
        "      id_filter_sql = \"msf.permno IN (\" + \",\".join(str(int(p)) for p in permnos) + \")\"\n",
        "    else:\n",
        "        tickers_sql = \",\".join(\"'\" + t.replace(\"'\", \"''\") + \"'\" for t in tickers)\n",
        "        id_filter_sql = f\"n.ticker IN ({tickers_sql})\"\n",
        "\n",
        "    q = f\"\"\"\n",
        "        SELECT\n",
        "            msf.permno,\n",
        "            msf.date,\n",
        "            msf.ret,\n",
        "            msf.shrout,\n",
        "            msf.prc,\n",
        "            n.shrcd,\n",
        "            n.exchcd,\n",
        "            n.ticker,\n",
        "            dl.dlret\n",
        "        FROM crsp.msf AS msf\n",
        "        JOIN crsp.msenames AS n\n",
        "          ON msf.permno = n.permno\n",
        "         AND n.namedt <= msf.date\n",
        "         AND (msf.date <= n.nameendt OR n.nameendt IS NULL)\n",
        "        LEFT JOIN crsp.msedelist AS dl\n",
        "          ON msf.permno = dl.permno\n",
        "         AND date_trunc('month', dl.dlstdt) = date_trunc('month', msf.date)\n",
        "        WHERE msf.date BETWEEN '{start_ymd}' AND '{end_ymd}'\n",
        "          AND n.shrcd IN (10, 11, 12)     -- US common shares/close equivalents\n",
        "          AND n.exchcd IN (1, 2, 3)       -- NYSE/AMEX/NASDAQ\n",
        "          AND {id_filter_sql}             -- CHANGE 3: use unified filter\n",
        "\n",
        "        ORDER BY msf.permno, msf.date;\n",
        "    \"\"\"\n",
        "\n",
        "    df = db.raw_sql(q, date_cols=[\"date\"])\n",
        "    if df.empty:\n",
        "        raise RuntimeError(\"CRSP query returned no rows. Check tickers and dates.\")\n",
        "\n",
        "    # cleanup\n",
        "    df[\"prc\"] = df[\"prc\"].abs()\n",
        "    df[\"ret\"] = pd.to_numeric(df[\"ret\"], errors=\"coerce\") # sets to NaN instead of error\n",
        "    df[\"dlret\"] = pd.to_numeric(df[\"dlret\"], errors=\"coerce\").fillna(0.0)\n",
        "\n",
        "    # incl delisting return: ret_total = (1+ret)*(1+dlret) - 1\n",
        "    df[\"ret_total\"] = (1.0 + df[\"ret\"]) * (1.0 + df[\"dlret\"]) - 1.0\n",
        "\n",
        "    # Month-end key for pivot\n",
        "    df[\"month\"] = pd.to_datetime(df[\"date\"]).dt.to_period(\"M\").dt.to_timestamp(\"M\")\n",
        "\n",
        "    # calc Market cap\n",
        "    df[\"shrout\"] = pd.to_numeric(df[\"shrout\"], errors=\"coerce\")\n",
        "    df[\"shrout_shares\"] = df[\"shrout\"] * 1000.0\n",
        "    df[\"me\"] = df[\"prc\"] * df[\"shrout_shares\"]\n",
        "\n",
        "    # drop rows w/missing returns\n",
        "    df = df.dropna(subset=[\"ret_total\"])\n",
        "\n",
        "    # Deduplicate (permno, month)\n",
        "    df = df.sort_values([\"permno\", \"month\"])\n",
        "    df = df.drop_duplicates(subset=[\"permno\", \"month\"], keep=\"last\")\n",
        "\n",
        "    # coverage - keep tickers with at least min_obs non-null returns\n",
        "    counts = df.groupby(\"ticker\")[\"ret_total\"].apply(lambda s: s.notna().sum())\n",
        "    kept = counts[counts >= min_obs].index.tolist()\n",
        "    dropped = sorted(set(df[\"ticker\"]) - set(kept))\n",
        "\n",
        "    if not kept:\n",
        "        raise RuntimeError(f\"All tickers dropped by coverage (min_obs={min_obs}).\")\n",
        "\n",
        "    df = df[df[\"ticker\"].isin(kept)].copy()\n",
        "\n",
        "    # Pivot to return matrix (rows=month-end, cols=tickers)\n",
        "    ret_matrix = (\n",
        "        df.pivot_table(index=\"month\", columns=\"ticker\", values=\"ret_total\", aggfunc=\"last\")\n",
        "          .sort_index()\n",
        "    )\n",
        "    ret_matrix.index = pd.to_datetime(ret_matrix.index).to_period(\"M\").to_timestamp(\"M\")\n",
        "\n",
        "    return ret_matrix, kept, dropped, df # need df later\n",
        "\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "def build_is_oos_matrices(\n",
        "    db,\n",
        "    tickers,\n",
        "    is_start, is_end,\n",
        "    oos_start, oos_end,\n",
        "    *,\n",
        "    min_obs_is: int = 24,\n",
        "    min_obs_oos: int = 1,\n",
        "    save_csv: bool = True,\n",
        "    outdir: str | Path = \"outputs\",\n",
        "    verbose: bool = True,\n",
        "):\n",
        "    \"\"\"\n",
        "    Build IS/OOS return matrices aligned by PERMNO (robust to ticker drift),\n",
        "    then relabel columns to IS tickers (last IS label per permno), clean zero-variance,\n",
        "    realign OOS to IS columns/order, and optionally save CSVs.\n",
        "\n",
        "    Returns a dict with:\n",
        "      - is_ret_matrix : IS matrix (rows=month_end, cols=tickers; cleaned)\n",
        "      - os_ret_matrix : OOS matrix (aligned to IS columns)\n",
        "      - permno_to_ticker : dict[int -> str] mapping (from IS, last label per permno)\n",
        "      - ret_perm : IS matrix by permno (before relabel/clean)\n",
        "      - oos_perm : OOS matrix by permno (aligned to IS permnos)\n",
        "      - df_is, df_oos : cleaned long frames from fetches\n",
        "      - kept_is, dropped_is, kept_oos, dropped_oos : lists\n",
        "    \"\"\"\n",
        "    # --- In-sample ---\n",
        "    ret_matrix, kept_is, dropped_is, df_is = fetch_crsp_returns(\n",
        "        db, tickers, is_start, is_end, min_obs=min_obs_is\n",
        "    )\n",
        "    if verbose:\n",
        "        print(\"IS matrix (raw):\", ret_matrix.shape, \"| kept:\", kept_is, \"| dropped:\", dropped_is)\n",
        "\n",
        "    # Resolve IS permnos from cleaned IS df\n",
        "    permnos_is = sorted(df_is[\"permno\"].dropna().astype(int).unique().tolist())\n",
        "    if verbose:\n",
        "        print(\"Resolved PERMNOs (IS):\", permnos_is)\n",
        "\n",
        "    # --- Out-of-sample (same permno universe; minimal coverage) ---\n",
        "    os_matrix, kept_oos, dropped_oos, df_oos = fetch_crsp_returns(\n",
        "        db, kept_is, oos_start, oos_end, min_obs=min_obs_oos, permnos=permnos_is\n",
        "    )\n",
        "\n",
        "    # --- Build permno-based matrices (robust to ticker string drift) ---\n",
        "    ret_perm = (\n",
        "        df_is.pivot_table(index=\"month\", columns=\"permno\", values=\"ret_total\", aggfunc=\"last\")\n",
        "            .sort_index()\n",
        "    )\n",
        "    oos_perm = (\n",
        "        df_oos.pivot_table(index=\"month\", columns=\"permno\", values=\"ret_total\", aggfunc=\"last\")\n",
        "            .sort_index()\n",
        "            .reindex(columns=ret_perm.columns)  # align OOS to IS permnos\n",
        "    )\n",
        "\n",
        "    # --- Map permno -> IS ticker (last label per permno in IS window) ---\n",
        "    permno_to_ticker = (\n",
        "        df_is.sort_values([\"permno\", \"month\"])\n",
        "             .drop_duplicates(subset=[\"permno\"], keep=\"last\")\n",
        "             .set_index(\"permno\")[\"ticker\"].astype(str).to_dict()\n",
        "    )\n",
        "\n",
        "    # Relabel columns for presentation\n",
        "    ret_matrix_tickers = ret_perm.rename(columns=permno_to_ticker)\n",
        "    os_ret_matrix_lbl  = oos_perm.rename(columns=permno_to_ticker)\n",
        "\n",
        "    if verbose:\n",
        "        print(\"IS (tickers):\", ret_matrix_tickers.shape)\n",
        "        print(\"OOS (tickers):\", os_ret_matrix_lbl.shape)\n",
        "\n",
        "    # --- Clean IS: drop all-NaN, then zero-variance columns ---\n",
        "    ret_is = ret_matrix_tickers.dropna(axis=1, how=\"all\").copy()\n",
        "    std_is = ret_is.std(skipna=True)\n",
        "    keep_cols = std_is[std_is > 0].index.tolist()\n",
        "    ret_is = ret_is[keep_cols]\n",
        "\n",
        "    # --- Realign OOS to cleaned IS columns/order ---\n",
        "    ret_oos = os_ret_matrix_lbl.reindex(columns=ret_is.columns)\n",
        "\n",
        "    # Name index and optionally save\n",
        "    ret_is.index.name  = \"month_end\"\n",
        "    ret_oos.index.name = \"month_end\"\n",
        "\n",
        "    if save_csv:\n",
        "        outdir = Path(outdir); outdir.mkdir(parents=True, exist_ok=True)\n",
        "        ret_is.to_csv(outdir / \"IS_ret_matrix.csv\",  float_format=\"%.8f\", na_rep=\"NA\")\n",
        "        ret_oos.to_csv(outdir / \"OOS_ret_matrix.csv\", float_format=\"%.8f\", na_rep=\"NA\")\n",
        "        if verbose:\n",
        "            print(\"Saved CSVs to:\", outdir)\n",
        "\n",
        "    return {\n",
        "        \"is_ret_matrix\": ret_is,\n",
        "        \"os_ret_matrix\": ret_oos,\n",
        "        \"permno_to_ticker\": permno_to_ticker,\n",
        "        \"ret_perm\": ret_perm,\n",
        "        \"oos_perm\": oos_perm,\n",
        "        \"df_is\": df_is,\n",
        "        \"df_oos\": df_oos,\n",
        "        \"kept_is\": kept_is,\n",
        "        \"dropped_is\": dropped_is,\n",
        "        \"kept_oos\": kept_oos,\n",
        "        \"dropped_oos\": dropped_oos,\n",
        "    }\n",
        "\n",
        "# get avg for each stock to demean later\n",
        "def get_mean(ret_matrix: pd.DataFrame) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Input:  ret_matrix = DataFrame of monthly simple returns (60 rows x N tickers).\n",
        "    Output: Series of average return per ticker (NaNs ignored).\n",
        "    \"\"\"\n",
        "    # If there are non-numeric cols, coerce them (optional; remove if not needed)\n",
        "    rets_num = ret_matrix.apply(pd.to_numeric, errors=\"coerce\")\n",
        "    return rets_num.mean(axis=0)  # monthly mean per column\n",
        "\n",
        "# monthly rawret - mean = monthly emat\n",
        "def demean_returns(ret_matrix: pd.DataFrame, means: pd.Series | None = None) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Input:  rets  = DataFrame of monthly returns (same shape as above)\n",
        "            means = (optional) Series of per-ticker means; if None, compute from rets\n",
        "    Output: DataFrame where each column has its mean subtracted (demeaned), also know as\n",
        "            the emat.\n",
        "    \"\"\"\n",
        "    rets_num = ret_matrix.apply(pd.to_numeric, errors=\"coerce\")\n",
        "    if means is None:\n",
        "        means = rets_num.mean(axis=0)\n",
        "    # Broadcast subtract per column; NaNs stay NaN\n",
        "    return rets_num.subtract(means, axis=1)\n",
        "\n",
        "def create_covmat(emat: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    E: DataFrame of demeaned returns (rows = time, cols = tickers), no NaNs.\n",
        "    Returns the sample covariance matrix using (E' E) / (n - 1).\n",
        "    \"\"\"\n",
        "    # ensure complete cases (Excel usually assumes same n per column here)\n",
        "    # maybe return error if not complete?\n",
        "    E = emat.dropna(axis=0, how=\"any\")\n",
        "    n = E.shape[0]\n",
        "    cov_np = (E.T @ E).to_numpy() / (n - 1)\n",
        "    return pd.DataFrame(cov_np, index=E.columns, columns=E.columns)\n",
        "\n",
        "def minverse(covmat: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Excel-like MINVERSE: returns (covmat)^(-1) as a DataFrame.\"\"\"\n",
        "    A = covmat.to_numpy(dtype=float)        # ensure float\n",
        "    I = np.eye(A.shape[0], dtype=float)\n",
        "    invA = np.linalg.solve(A, I)            # solves A * X = I => X = A' * I\n",
        "    return pd.DataFrame(invA, index=covmat.index, columns=covmat.columns)\n",
        "\n",
        "\n",
        "def get_N_T(emat: pd.DataFrame) -> tuple[int, int]:\n",
        "    \"\"\"\n",
        "    N = number of tickers (columns), T = number of rows (after complete-case drop)\n",
        "    \"\"\"\n",
        "    E = emat.dropna(axis=0, how=\"any\")\n",
        "    T = E.shape[0]\n",
        "    N = E.shape[1]\n",
        "    return N, T\n",
        "\n",
        "def get_variances(emat: pd.DataFrame) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Column-wise sample variances of demeaned returns.\n",
        "    Assumes emat is already demeaned (mean ~ 0 per column).\n",
        "    \"\"\"\n",
        "    E = emat.dropna(axis=0, how=\"any\")\n",
        "    return E.var(axis=0, ddof=1)\n",
        "\n",
        "def avg_variance(emat: pd.DataFrame) -> float:\n",
        "    \"\"\"\n",
        "    Average of column variances (i.e., trace(S)/N).\n",
        "    \"\"\"\n",
        "    return float(get_variances(emat).mean())\n",
        "\n",
        "def port_stdev(w, c):\n",
        "  return (float(w.T @ c @ w))**0.5\n",
        "\n",
        "def create_betas(emat: pd.DataFrame, covmat: pd.DataFrame) -> pd.Series:\n",
        "    \"\"\"\n",
        "    For each row t of emat, compute:\n",
        "      SSE_t = sum( ( x_t x_t' - covmat )^2 )\n",
        "    where x_t is the 1×N demeaned return vector at time t.\n",
        "    Returns a Series indexed like emat.index.\n",
        "    \"\"\"\n",
        "    E = emat.dropna(axis=0, how=\"any\").to_numpy(dtype=float)  # T×N\n",
        "    S = covmat.reindex(index=emat.columns, columns=emat.columns).to_numpy(dtype=float)  # N×N\n",
        "\n",
        "    sse_list = []\n",
        "    for x in E:  # x is shape (N,)\n",
        "        outer = np.outer(x, x)        # N×N\n",
        "        diff = outer - S\n",
        "        sse_list.append(float(np.sum(diff * diff)))  # Frobenius norm squared\n",
        "\n",
        "    return pd.Series(sse_list, index=emat.dropna(axis=0, how=\"any\").index, name=\"row_sse\")\n",
        "\n",
        "def create_ident(N: int, avg_var: float, columns: list[str] | None = None) -> pd.DataFrame:\n",
        "    I = np.eye(N) * avg_var\n",
        "    if columns is None:\n",
        "        return pd.DataFrame(I)\n",
        "    return pd.DataFrame(I, index=columns, columns=columns)\n",
        "\n",
        "def delta_squared(covmat: pd.DataFrame, ident: pd.DataFrame) -> float:\n",
        "    \"\"\"\n",
        "    Returns SUM((covmat - ident)^2) with index/column alignment.\n",
        "    \"\"\"\n",
        "    # align to common order (and error if mismatch)\n",
        "    cov = covmat.copy()\n",
        "    idt = ident.reindex(index=cov.index, columns=cov.columns)\n",
        "    if idt.isnull().values.any():\n",
        "        raise ValueError(\"ident must have the same index/columns as covmat.\")\n",
        "\n",
        "    diff = cov.to_numpy(dtype=float) - idt.to_numpy(dtype=float)\n",
        "    return float(np.sum(diff * diff))  # Frobenius norm squared\n",
        "\n",
        "def beta_squared(betas, T: int) -> float:\n",
        "    if T <= 0:\n",
        "        raise ValueError(\"T must be > 0.\")\n",
        "    return float(np.nansum(np.asarray(betas, float))) / (T**2)\n",
        "\n",
        "def get_lambda(emat: pd.DataFrame, covmat: pd.DataFrame) -> float:\n",
        "    \"\"\"\n",
        "    λ = clip( β² / δ², 0, 1 ), with β² = mean row SSEs from create_betas.\n",
        "    \"\"\"\n",
        "    ident = create_ident(N, avg_var, columns=emat.columns)\n",
        "    betas = create_betas(emat, covmat)\n",
        "    beta2 = beta_squared(betas,T)\n",
        "    delta2 = delta_squared(covmat, ident)\n",
        "    if delta2 <= 0:\n",
        "        return 0.0\n",
        "    lam = beta2 / delta2\n",
        "    return float(min(1.0, max(0.0, lam)))\n",
        "\n",
        "def calc_lw_covmat(covmat: pd.DataFrame, ident: pd.DataFrame, lam: float) -> pd.DataFrame:\n",
        "\n",
        "    # Align labels\n",
        "    ident_aligned = ident.reindex(index=covmat.index, columns=covmat.columns)\n",
        "    if ident_aligned.isnull().values.any():\n",
        "        raise ValueError(\"ident must have the same index/columns as covmat.\")\n",
        "\n",
        "    return lam * ident_aligned + (1.0 - lam) * covmat\n",
        "\n",
        "def portfolio_return(weights: pd.Series, means: pd.Series) -> float:\n",
        "    \"\"\"\n",
        "    Computes MMULT(TRANSPOSE(weights), means).\n",
        "    Both inputs indexed by the same tickers.\n",
        "    Returns a scalar (same period as `means`).\n",
        "    \"\"\"\n",
        "    w = pd.to_numeric(weights, errors=\"coerce\")\n",
        "    m = pd.to_numeric(means, errors=\"coerce\")\n",
        "    m = m.reindex(w.index)   # align by ticker names\n",
        "    return float(np.dot(w.values, m.values))\n",
        "\n",
        "def closedformgmv(invcovmat: pd.DataFrame) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Global Minimum Variance (GMV) weights given INVCOVMAT.\n",
        "    Numerator:   INVCOVMAT * ONEVEC\n",
        "    Denominator: 1' * INVCOVMAT * 1\n",
        "    \"\"\"\n",
        "    Ainv = invcovmat.to_numpy(dtype=float)\n",
        "    n = Ainv.shape[0]\n",
        "    one = np.ones((n, 1))\n",
        "    num = Ainv @ one\n",
        "    den = float(one.T @ Ainv @ one)\n",
        "    w = (num / den).ravel()\n",
        "    return pd.Series(w, index=invcovmat.index)  # sums to ~1\n",
        "\n",
        "def calc_efficientportfolio(invcovmat: pd.DataFrame, means: pd.Series, mu0: float) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Unconstrained efficient-portfolio weights for target return mu0.\n",
        "    Uses: w = P[(c - b*mu0)/d * 1 + (a*mu0 - b)/d * mu],\n",
        "    where P = invcovmat, a=1'P1, b=1'Pmu, c=mu'Pmu, d=ac-b^2.\n",
        "    \"\"\"\n",
        "    P   = invcovmat.to_numpy(dtype=float)\n",
        "    mu  = means.reindex(invcovmat.index).to_numpy(dtype=float).reshape(-1, 1)\n",
        "    one = np.ones((P.shape[0], 1))\n",
        "\n",
        "    a = float(one.T @ P @ one)\n",
        "    b = float(one.T @ P @ mu)\n",
        "    c = float(mu.T  @ P @ mu)\n",
        "    d = a * c - b * b\n",
        "\n",
        "    w = ((c - b*mu0)/d) * (P @ one) + ((a*mu0 - b)/d) * (P @ mu)\n",
        "    w = w.ravel()\n",
        "    w = w / w.sum()  # tidy sum-to-one\n",
        "\n",
        "    return pd.Series(w, index=invcovmat.index, name=\"Eff_Weight\")\n",
        "\n",
        "\n",
        "def calc_orp(invcovmat: pd.DataFrame, means: pd.Series, rfrate: float) -> pd.Series:\n",
        "      \"\"\"\n",
        "      Optimal Risky Portfolio (tangency) weights.\n",
        "      Excel analog:\n",
        "        Numerator   = MMULT(INVCOVMAT, muvec - rfrate)\n",
        "        Denominator = MMULT(TRANSPOSE(ONEVEC), MMULT(INVCOVMAT, muvec - rfrate))\n",
        "      Returns weights that sum to 1.\n",
        "      \"\"\"\n",
        "      # Align and cast\n",
        "      mu = means.reindex(invcovmat.index).to_numpy(dtype=float).reshape(-1, 1)\n",
        "      P  = invcovmat.to_numpy(dtype=float)\n",
        "      e  = np.ones((P.shape[0], 1))\n",
        "\n",
        "      x = P @ (mu - rfrate * e)            # numerator vector\n",
        "      denom = float(e.T @ x)               # scalar denominator\n",
        "      w = (x / denom).ravel()              # normalize to sum to 1\n",
        "      return pd.Series(w, index=invcovmat.index, name=\"ORP_Weight\")\n",
        "\n",
        "def calc_orp_scipy(covmat: pd.DataFrame, means: pd.Series, rfrate: float,\n",
        "                   lb: float = -1e6, ub: float = 1e6, ftol: float = 1e-9, maxiter: int = 2000) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Tangency (ORP) via SLSQP with box bounds lb <= w_i <= ub and sum(w)=1.\n",
        "    Maximizes Sharpe: (w·mu - rf) / sqrt(w'Σw).\n",
        "    \"\"\"\n",
        "    # Align inputs\n",
        "    assert list(covmat.index) == list(covmat.columns), \"covmat index/columns mismatch\"\n",
        "    mu = means.reindex(covmat.index).to_numpy(dtype=float)\n",
        "    Σ  = covmat.to_numpy(dtype=float)\n",
        "    n  = len(mu)\n",
        "\n",
        "    # Feasibility: sum(w)=1 with box bounds\n",
        "    if lb > ub:\n",
        "        raise ValueError(\"lb > ub\")\n",
        "    if n*lb - 1.0 > 1e-12 or 1.0 - n*ub > 1e-12:\n",
        "        raise ValueError(f\"Infeasible bounds for sum=1: need n*lb <= 1 <= n*ub (n={n}, lb={lb}, ub={ub}).\")\n",
        "\n",
        "    # Objective (minimize negative Sharpe)\n",
        "    def neg_sharpe(w: np.ndarray) -> float:\n",
        "        pr = float(w @ mu)\n",
        "        v2 = float(w @ Σ @ w)\n",
        "        if v2 <= 0:\n",
        "            return 1e12\n",
        "        return -(pr - rfrate) / (v2 ** 0.5)\n",
        "\n",
        "    cons   = ({'type': 'eq', 'fun': lambda w: np.sum(w) - 1.0},)\n",
        "    bounds = [(lb, ub)] * n\n",
        "    w0     = np.full(n, 1.0 / n)\n",
        "\n",
        "    res = minimize(neg_sharpe, w0, method='SLSQP', bounds=bounds, constraints=cons,\n",
        "                   options={'ftol': ftol, 'maxiter': maxiter})\n",
        "    if not res.success:\n",
        "        raise RuntimeError(f\"Optimization failed: {res.message}\")\n",
        "    return pd.Series(res.x, index=covmat.index, name=\"ORP_Weight\")\n",
        "\n",
        "\n",
        "# for excel\n",
        "def save_current_fig(path):\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(path, dpi=200, bbox_inches=\"tight\")\n",
        "    print(f\"[Plot] Saved {path}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Call Functions\n",
        "\n",
        "#Get user inputs\n",
        "dates = get_dates()\n",
        "is_start  = dates[0]\n",
        "is_end    = dates[1]\n",
        "oos_start = dates[2]\n",
        "oos_end   = dates[3]\n",
        "tickers = get_tickers(all_ticker_set)\n",
        "weights = get_portfolio_weights(tickers)\n",
        "\n",
        "#Build returns\n",
        "res = build_is_oos_matrices(db, tickers,\n",
        "                            is_start, is_end, oos_start, oos_end,\n",
        "                            min_obs_is=24, min_obs_oos=1, save_csv=True)\n",
        "\n",
        "ret_matrix = res[\"is_ret_matrix\"]\n",
        "os_ret_matrix = res[\"os_ret_matrix\"]\n"
      ],
      "metadata": {
        "id": "sFVG5DJ1xcak",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb4eaf7e-2112-44a5-ce6e-5241f1a7ff0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In-sample start (YYYY-MM): 2014-01\n",
            "In-sample end   (YYYY-MM): 2020-12\n",
            "Out-of-sample start (YYYY-MM): 2021-01\n",
            "Out-of-sample end   (YYYY-MM): 2023-12\n",
            "Enter tickers separated by commas (e.g., AAPL, MSFT, TSLA) or quit to exit:DE, LMT, DIS, MCD, WMT, NKE, LUV, AMD, AMZN, TSLA\n",
            "What is your original portfolio weight for each ticker, in order, separated by commas? Enter 10 for 10%, equal for equal weights, or quit to exit\n",
            "equal\n",
            "IS matrix (raw): (84, 10) | kept: ['AMD', 'AMZN', 'DE', 'DIS', 'LMT', 'LUV', 'MCD', 'NKE', 'TSLA', 'WMT'] | dropped: []\n",
            "Resolved PERMNOs (IS): [19350, 21178, 26403, 43449, 55976, 57665, 58683, 61241, 84788, 93436]\n",
            "IS (tickers): (84, 10)\n",
            "OOS (tickers): (36, 10)\n",
            "Saved CSVs to: outputs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Get means\n",
        "means = get_mean(ret_matrix)\n",
        "os_means = get_mean(os_ret_matrix)\n",
        "\n",
        "#Get Emats\n",
        "emat = demean_returns(ret_matrix,means)\n",
        "os_emat = demean_returns(os_ret_matrix,os_means)\n",
        "\n",
        "#Create Covmats\n",
        "covmat = create_covmat(emat)\n",
        "os_covmat = create_covmat(os_emat)\n",
        "\n",
        "invcovmat = minverse(covmat)\n",
        "\n",
        "#Get Ledoit Wolf Features\n",
        "N,T = get_N_T(emat)\n",
        "betas = create_betas(emat, covmat)\n",
        "avg_var = avg_variance(emat)\n",
        "ident = create_ident(N, avg_var, columns=emat.columns)\n",
        "lam = get_lambda(emat, covmat)\n",
        "lw_covmat = calc_lw_covmat(covmat, ident, lam)\n",
        "os_lw_covmat = calc_lw_covmat(os_covmat, ident, lam)\n",
        "lwinvcovmat = minverse(lw_covmat)\n"
      ],
      "metadata": {
        "id": "1V2mgzdZyeBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rfrate = .035/12\n",
        "mu0 = portfolio_return(weights,means)\n",
        "\n",
        "#Get GMVs\n",
        "gmv = closedformgmv(invcovmat)\n",
        "lwgmv = closedformgmv(lwinvcovmat)\n",
        "\n",
        "#Get Efficient Portfolios\n",
        "efport = calc_efficientportfolio(invcovmat, means, mu0)\n",
        "lwefport = calc_efficientportfolio(lwinvcovmat,means,mu0)\n",
        "\n",
        "#Get Regular ORP\n",
        "orp = calc_orp(invcovmat,means,rfrate)\n",
        "lworp = calc_orp(lwinvcovmat,means,rfrate)\n",
        "\n",
        "#Get quadratic programming orp\n",
        "qporp = calc_orp_scipy(covmat, means, rfrate, lb=-1.0, ub=1.0)\n",
        "lwqporp = calc_orp_scipy(lw_covmat, means, rfrate, lb=-1.0, ub=1.0)\n"
      ],
      "metadata": {
        "id": "X79HFxfc08A2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edb29206-c39f-4151-97ee-f53942d424e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4073525548.py:573: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  den = float(one.T @ Ainv @ one)\n",
            "/tmp/ipython-input-4073525548.py:587: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  a = float(one.T @ P @ one)\n",
            "/tmp/ipython-input-4073525548.py:588: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  b = float(one.T @ P @ mu)\n",
            "/tmp/ipython-input-4073525548.py:589: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  c = float(mu.T  @ P @ mu)\n",
            "/tmp/ipython-input-4073525548.py:587: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  a = float(one.T @ P @ one)\n",
            "/tmp/ipython-input-4073525548.py:588: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  b = float(one.T @ P @ mu)\n",
            "/tmp/ipython-input-4073525548.py:589: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  c = float(mu.T  @ P @ mu)\n",
            "/tmp/ipython-input-4073525548.py:613: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  denom = float(e.T @ x)               # scalar denominator\n",
            "/tmp/ipython-input-4073525548.py:613: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  denom = float(e.T @ x)               # scalar denominator\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== In-Sample Portfolio Weights (regular Σ) ===\")\n",
        "print(\"Original Portfolio Weights:\")\n",
        "print(weights, \"\\n\")\n",
        "\n",
        "print(\"GMV Weights:\")\n",
        "print(gmv, \"\\n\")\n",
        "\n",
        "print(\"Efficient Portfolio Weights:\")\n",
        "print(efport, \"\\n\")\n",
        "\n",
        "print(\"ORP Weights:\")\n",
        "print(orp, \"\\n\")\n",
        "\n",
        "print(\"QP ORP Weights:\")\n",
        "print(qporp, \"\\n\")\n",
        "\n",
        "\n",
        "print(\"\\n=== In-Sample Portfolio Weights (Ledoit–Wolf Σ) ===\")\n",
        "print(\"LW GMV Weights:\")\n",
        "print(lwgmv, \"\\n\")\n",
        "\n",
        "print(\"LW Efficient Portfolio Weights:\")\n",
        "print(lwefport, \"\\n\")\n",
        "\n",
        "print(\"LW ORP Weights:\")\n",
        "print(lworp, \"\\n\")\n",
        "\n",
        "print(\"LW QP ORP Weights:\")\n",
        "print(lwqporp, \"\\n\")\n"
      ],
      "metadata": {
        "id": "uhzXKyRDnryD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e737d2b-e968-4ec1-ba8d-0693eed0a5f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== In-Sample Portfolio Weights (regular Σ) ===\n",
            "Original Portfolio Weights:\n",
            "DE      0.1\n",
            "LMT     0.1\n",
            "DIS     0.1\n",
            "MCD     0.1\n",
            "WMT     0.1\n",
            "NKE     0.1\n",
            "LUV     0.1\n",
            "AMD     0.1\n",
            "AMZN    0.1\n",
            "TSLA    0.1\n",
            "dtype: float64 \n",
            "\n",
            "GMV Weights:\n",
            "permno\n",
            "DE      0.086199\n",
            "LMT     0.172532\n",
            "DIS     0.003709\n",
            "MCD     0.315040\n",
            "WMT     0.294299\n",
            "NKE     0.145442\n",
            "LUV    -0.007563\n",
            "AMD    -0.034402\n",
            "AMZN    0.039580\n",
            "TSLA   -0.014837\n",
            "dtype: float64 \n",
            "\n",
            "Efficient Portfolio Weights:\n",
            "permno\n",
            "DE      0.073005\n",
            "LMT     0.124374\n",
            "DIS    -0.163064\n",
            "MCD     0.218156\n",
            "WMT     0.147775\n",
            "NKE     0.270910\n",
            "LUV     0.004357\n",
            "AMD     0.082534\n",
            "AMZN    0.149984\n",
            "TSLA    0.091968\n",
            "Name: Eff_Weight, dtype: float64 \n",
            "\n",
            "ORP Weights:\n",
            "permno\n",
            "DE      0.066097\n",
            "LMT     0.099163\n",
            "DIS    -0.250371\n",
            "MCD     0.167437\n",
            "WMT     0.071069\n",
            "NKE     0.336593\n",
            "LUV     0.010597\n",
            "AMD     0.143751\n",
            "AMZN    0.207782\n",
            "TSLA    0.147881\n",
            "Name: ORP_Weight, dtype: float64 \n",
            "\n",
            "QP ORP Weights:\n",
            "permno\n",
            "DE      0.066092\n",
            "LMT     0.099172\n",
            "DIS    -0.250380\n",
            "MCD     0.167452\n",
            "WMT     0.071093\n",
            "NKE     0.336593\n",
            "LUV     0.010579\n",
            "AMD     0.143756\n",
            "AMZN    0.207764\n",
            "TSLA    0.147881\n",
            "Name: ORP_Weight, dtype: float64 \n",
            "\n",
            "\n",
            "=== In-Sample Portfolio Weights (Ledoit–Wolf Σ) ===\n",
            "LW GMV Weights:\n",
            "permno\n",
            "DE      0.098638\n",
            "LMT     0.166003\n",
            "DIS     0.042896\n",
            "MCD     0.247032\n",
            "WMT     0.265463\n",
            "NKE     0.139975\n",
            "LUV     0.016803\n",
            "AMD    -0.030850\n",
            "AMZN    0.062942\n",
            "TSLA   -0.008901\n",
            "dtype: float64 \n",
            "\n",
            "LW Efficient Portfolio Weights:\n",
            "permno\n",
            "DE      0.081363\n",
            "LMT     0.114817\n",
            "DIS    -0.074382\n",
            "MCD     0.166944\n",
            "WMT     0.154031\n",
            "NKE     0.203869\n",
            "LUV     0.022967\n",
            "AMD     0.085227\n",
            "AMZN    0.150726\n",
            "TSLA    0.094437\n",
            "Name: Eff_Weight, dtype: float64 \n",
            "\n",
            "LW ORP Weights:\n",
            "permno\n",
            "DE      0.070179\n",
            "LMT     0.081678\n",
            "DIS    -0.150310\n",
            "MCD     0.115092\n",
            "WMT     0.081886\n",
            "NKE     0.245235\n",
            "LUV     0.026958\n",
            "AMD     0.160379\n",
            "AMZN    0.207561\n",
            "TSLA    0.161342\n",
            "Name: ORP_Weight, dtype: float64 \n",
            "\n",
            "LW QP ORP Weights:\n",
            "permno\n",
            "DE      0.070194\n",
            "LMT     0.081702\n",
            "DIS    -0.150318\n",
            "MCD     0.115077\n",
            "WMT     0.081874\n",
            "NKE     0.245232\n",
            "LUV     0.026957\n",
            "AMD     0.160378\n",
            "AMZN    0.207559\n",
            "TSLA    0.161344\n",
            "Name: ORP_Weight, dtype: float64 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Build Returns\n",
        "pfs = pd.DataFrame(columns=[\"Expected Return\",\"Standard Deviation\"])\n",
        "pfs.loc['IS Orig Port'] = [portfolio_return(weights, means), port_stdev(weights, covmat)]\n",
        "pfs.loc['IS GMV'] = [portfolio_return(gmv, means), port_stdev(gmv, covmat)]\n",
        "pfs.loc['IS Efficient Portfolio'] = [portfolio_return(efport, means), port_stdev(efport, covmat)]\n",
        "pfs.loc['IS ORP'] = [portfolio_return(orp, means), port_stdev(orp, covmat)]\n",
        "pfs.loc['IS QP ORP'] = [portfolio_return(qporp, means), port_stdev(qporp, covmat)]\n",
        "\n",
        "pfs.loc['OS Orig Port']  = [portfolio_return(weights, os_means), port_stdev(weights, os_covmat)]\n",
        "pfs.loc['OS GMV'] = [portfolio_return(gmv, os_means), port_stdev(gmv, os_covmat)]\n",
        "pfs.loc['OS Efficient Portfolio'] = [portfolio_return(efport, os_means), port_stdev(efport, os_covmat)]\n",
        "pfs.loc['OS ORP'] = [portfolio_return(orp, os_means), port_stdev(orp, os_covmat)]\n",
        "pfs.loc['OS QP ORP'] = [portfolio_return(qporp, os_means), port_stdev(qporp, os_covmat)]\n",
        "\n",
        "pfs['Sharpe Ratio (Monthly)'] = (pfs['Expected Return']-rfrate)/pfs['Standard Deviation']\n",
        "pfs['Sharpe Ratio (Annualized)'] = (pfs['Sharpe Ratio (Monthly)'])*(12**0.5)\n",
        "\n",
        "# pfs['Expected Return (Annualized)'] = pfs['Expected Return'] * 12\n",
        "# pfs['Standard Deviation (Annualized)'] = pfs['Standard Deviation'] * (12 ** 0.5)\n",
        "\n",
        "pfslw = pd.DataFrame(columns=[\"Expected Return\",\"Standard Deviation\"])\n",
        "pfslw.loc['IS LW Orig Port'] = [portfolio_return(weights,    means),    port_stdev(weights,    lw_covmat)]\n",
        "pfslw.loc['IS LW GMV']       = [portfolio_return(lwgmv,     means),    port_stdev(lwgmv,       lw_covmat)]\n",
        "pfslw.loc['IS LW Efficient'] = [portfolio_return(lwefport,  means),    port_stdev(lwefport,    lw_covmat)]\n",
        "pfslw.loc['IS LW ORP']       = [portfolio_return(lworp,     means),    port_stdev(lworp,       lw_covmat)]\n",
        "pfslw.loc['IS LW QP ORP']    = [portfolio_return(lwqporp,   means),    port_stdev(lwqporp,     lw_covmat)]\n",
        "\n",
        "pfslw.loc['OS LW Orig Port'] = [portfolio_return(weights,    os_means),    port_stdev(weights,    os_lw_covmat)]\n",
        "pfslw.loc['OS LW GMV']       = [portfolio_return(lwgmv,     os_means),    port_stdev(lwgmv,       os_lw_covmat)]\n",
        "pfslw.loc['OS LW Efficient'] = [portfolio_return(lwefport,  os_means),    port_stdev(lwefport,    os_lw_covmat)]\n",
        "pfslw.loc['OS LW ORP']       = [portfolio_return(lworp,     os_means),    port_stdev(lworp,       os_lw_covmat)]\n",
        "pfslw.loc['OS LW QP ORP']    = [portfolio_return(lwqporp,   os_means),    port_stdev(lwqporp,     os_lw_covmat)]\n",
        "\n",
        "pfslw['Sharpe Ratio (Monthly)'] = (pfslw['Expected Return']-rfrate)/pfslw['Standard Deviation']\n",
        "pfslw['Sharpe Ratio (Annualized)'] = (pfslw['Sharpe Ratio (Monthly)'])*(12**0.5)\n",
        "\n",
        "# pfslw['Expected Return (Annualized)'] = pfs['Expected Return'] * 12\n",
        "# pfslw['Standard Deviation (Annualized)'] = pfs['Standard Deviation'] * (12 ** 0.5)\n",
        "\n",
        "print(pfs)\n",
        "print(pfslw)\n",
        "\n",
        "#Call Graphs\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dpHvQPTdxafV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41e4acd0-f819-46c3-b3ca-373d427bb884"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                        Expected Return  Standard Deviation  \\\n",
            "IS Orig Port                   0.023423            0.052088   \n",
            "IS GMV                         0.012308            0.035956   \n",
            "IS Efficient Portfolio         0.023423            0.047928   \n",
            "IS ORP                         0.029241            0.060198   \n",
            "IS QP ORP                      0.029241            0.060198   \n",
            "OS Orig Port                   0.006700            0.062813   \n",
            "OS GMV                         0.007333            0.047159   \n",
            "OS Efficient Portfolio         0.011918            0.058834   \n",
            "OS ORP                         0.014318            0.074441   \n",
            "OS QP ORP                      0.014319            0.074440   \n",
            "\n",
            "                        Sharpe Ratio (Monthly)  Sharpe Ratio (Annualized)  \n",
            "IS Orig Port                          0.393682                   1.363754  \n",
            "IS GMV                                0.261200                   0.904823  \n",
            "IS Efficient Portfolio                0.427854                   1.482129  \n",
            "IS ORP                                0.437302                   1.514859  \n",
            "IS QP ORP                             0.437302                   1.514859  \n",
            "OS Orig Port                          0.060233                   0.208653  \n",
            "OS GMV                                0.093637                   0.324367  \n",
            "OS Efficient Portfolio                0.152993                   0.529984  \n",
            "OS ORP                                0.153164                   0.530577  \n",
            "OS QP ORP                             0.153174                   0.530612  \n",
            "                 Expected Return  Standard Deviation  Sharpe Ratio (Monthly)  \\\n",
            "IS LW Orig Port         0.023423            0.049875                0.411156   \n",
            "IS LW GMV               0.013232            0.037334                0.276302   \n",
            "IS LW Efficient         0.023423            0.047219                0.434276   \n",
            "IS LW ORP               0.030021            0.060517                0.447875   \n",
            "IS LW QP ORP            0.030021            0.060517                0.447875   \n",
            "OS LW Orig Port         0.006700            0.059677                0.063398   \n",
            "OS LW GMV               0.006015            0.047103                0.065780   \n",
            "OS LW Efficient         0.010183            0.056607                0.128369   \n",
            "OS LW ORP               0.012882            0.073130                0.136267   \n",
            "OS LW QP ORP            0.012882            0.073130                0.136273   \n",
            "\n",
            "                 Sharpe Ratio (Annualized)  \n",
            "IS LW Orig Port                   1.424285  \n",
            "IS LW GMV                         0.957138  \n",
            "IS LW Efficient                   1.504377  \n",
            "IS LW ORP                         1.551483  \n",
            "IS LW QP ORP                      1.551483  \n",
            "OS LW Orig Port                   0.219617  \n",
            "OS LW GMV                         0.227870  \n",
            "OS LW Efficient                   0.444683  \n",
            "OS LW ORP                         0.472041  \n",
            "OS LW QP ORP                      0.472063  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from matplotlib.lines import Line2D\n",
        "import matplotlib.patheffects as pe\n",
        "\n",
        "HALO = [pe.withStroke(linewidth=3, foreground=\"white\")]  # outline around text\n",
        "\n",
        "def efficient_frontier_unconstrained(means: pd.Series, covmat: pd.DataFrame, n: int = 300):\n",
        "    idx = covmat.index\n",
        "    mu  = means.reindex(idx).to_numpy(dtype=float).reshape(-1, 1)\n",
        "    P   = np.linalg.inv(covmat.to_numpy(dtype=float))\n",
        "    one = np.ones_like(mu)\n",
        "    a = float(one.T @ P @ one); b = float(one.T @ P @ mu); c = float(mu.T @ P @ mu)\n",
        "    d = a * c - b * b\n",
        "    mu_gmv = b / a\n",
        "    mu_hi  = max(float(means.max()), mu_gmv) * 1.2\n",
        "    mus = np.linspace(mu_gmv, mu_hi, n)\n",
        "    sig2 = (a*mus**2 - 2*b*mus + c) / d\n",
        "    sig  = np.sqrt(np.maximum(sig2, 0.0))\n",
        "    return sig, mus\n",
        "\n",
        "def _letter_map_from_index(index_like) -> dict:\n",
        "    # A, B, C, ... Z, AA, AB, ...\n",
        "    letters, k = [], 0\n",
        "    for _ in range(len(index_like)):\n",
        "        n, s = k, \"\"\n",
        "        while True:\n",
        "            s = chr(ord('A') + (n % 26)) + s\n",
        "            n = n // 26 - 1\n",
        "            if n < 0: break\n",
        "        letters.append(s); k += 1\n",
        "    return {name: lab for name, lab in zip(index_like, letters)}\n",
        "\n",
        "def _add_letter_legend(ax, mapping, title=\"Letter → Portfolio\", loc=\"lower right\"):\n",
        "    labels = [f\"{lab}: {name}\" for name, lab in mapping.items()]\n",
        "    handles = [Line2D([0],[0], color='none') for _ in labels]  # invisible handles\n",
        "    leg = ax.legend(handles, labels, title=title, frameon=True, loc=loc)\n",
        "    ax.add_artist(leg)\n",
        "\n",
        "def _annotate_static(ax, df, mask, color, letter_map, dy_pixels):\n",
        "    # Fixed offset labels, no arrows, with a white bubble + halo to stand out\n",
        "    for name, row in df.loc[mask].iterrows():\n",
        "        x, y = row['Standard Deviation'], row['Expected Return']\n",
        "        if not (np.isfinite(x) and np.isfinite(y)):\n",
        "            continue\n",
        "        label = letter_map.get(name, name)\n",
        "        ax.annotate(\n",
        "            label, (x, y),\n",
        "            xytext=(0, dy_pixels), textcoords=\"offset points\",\n",
        "            ha=\"center\", va=(\"bottom\" if dy_pixels > 0 else \"top\"),\n",
        "            fontsize=10, fontweight=\"bold\", color=color,\n",
        "            bbox=dict(boxstyle=\"round,pad=0.25\", fc=\"white\", ec=\"none\", alpha=0.9),\n",
        "            path_effects=HALO, zorder=6, clip_on=False\n",
        "        )\n",
        "\n",
        "def plot_is_os_frontier(\n",
        "    pfs: pd.DataFrame, means: pd.Series, covmat: pd.DataFrame, rfrate: float, title: str,\n",
        "    color_is='#1f77b4', color_os='#ff7f0e', cal_color='#2ca02c', frontier_color='#17becf',\n",
        "    use_letters: bool = True, letter_legend_loc: str = \"lower right\"\n",
        "):\n",
        "    # numeric guard\n",
        "    for col in ['Standard Deviation', 'Expected Return']:\n",
        "        pfs[col] = pd.to_numeric(pfs[col], errors='coerce')\n",
        "\n",
        "    idx_norm = pfs.index.astype(str).str.lower().str.strip()\n",
        "    is_mask = idx_norm.str.startswith('is ')\n",
        "    os_mask = idx_norm.str.startswith('os ')\n",
        "    letter_map = _letter_map_from_index(pfs.index.tolist()) if use_letters else {}\n",
        "\n",
        "    sns.set(style='whitegrid')\n",
        "    fig, ax = plt.subplots(figsize=(8.5,6.5), dpi=160)\n",
        "\n",
        "    # Points with white edge for contrast\n",
        "    if pfs.loc[is_mask].size:\n",
        "        sub = pfs.loc[is_mask, ['Standard Deviation','Expected Return']].dropna()\n",
        "        ax.scatter(sub['Standard Deviation'], sub['Expected Return'],\n",
        "                   c=color_is, s=110, label='IS', zorder=4,\n",
        "                   edgecolors=\"white\", linewidths=1.2)\n",
        "        _annotate_static(ax, pfs, is_mask, color_is, letter_map, dy_pixels=12)\n",
        "\n",
        "    if pfs.loc[os_mask].size:\n",
        "        sub = pfs.loc[os_mask, ['Standard Deviation','Expected Return']].dropna()\n",
        "        ax.scatter(sub['Standard Deviation'], sub['Expected Return'],\n",
        "                   c=color_os, s=130, marker='^', label='OOS', zorder=4,\n",
        "                   edgecolors=\"white\", linewidths=1.2)\n",
        "        _annotate_static(ax, pfs, os_mask, color_os, letter_map, dy_pixels=-14)\n",
        "\n",
        "    # CAL via \"IS QP ORP\" (full line kept)\n",
        "    tag = 'is qp orp'\n",
        "    poss = idx_norm[idx_norm.str.contains(tag)].tolist()\n",
        "    if poss:\n",
        "        label = pfs.index[idx_norm == poss[0]][0]\n",
        "        sd = float(pfs.loc[label, 'Standard Deviation']); er = float(pfs.loc[label, 'Expected Return'])\n",
        "        if np.isfinite(sd) and np.isfinite(er):\n",
        "            slope = (er - rfrate)/max(sd, 1e-12)\n",
        "            x_max = max(float(pfs['Standard Deviation'].max())*1.25, sd*1.35)\n",
        "            xs = np.linspace(0, x_max, 400); ys = rfrate + slope*xs\n",
        "            ax.plot(xs, ys, color=cal_color, linestyle='--', linewidth=2.4,\n",
        "                    label=f'CAL via {label}', zorder=2.5)\n",
        "            ax.scatter([0],[rfrate], color=cal_color, edgecolor='k', zorder=5)\n",
        "\n",
        "    # Efficient frontier (keep full line)\n",
        "    sig, mu = efficient_frontier_unconstrained(means, covmat, n=400)\n",
        "    if sig.size and mu.size:\n",
        "        ax.plot(sig, mu, color=frontier_color, linewidth=2.8,\n",
        "                label='Efficient Frontier (IS)', zorder=2)\n",
        "\n",
        "    # Limits from both points & frontier (no clipping)\n",
        "    all_x = np.concatenate([pfs['Standard Deviation'].dropna().values, sig])\n",
        "    all_y = np.concatenate([pfs['Expected Return'].dropna().values,     mu ])\n",
        "    xpad = (all_x.max() - all_x.min()) * 0.08 if all_x.size else 0.01\n",
        "    ypad = (all_y.max() - all_y.min()) * 0.08 if all_y.size else 0.01\n",
        "    ax.set_xlim(all_x.min() - xpad, all_x.max() + xpad)\n",
        "    ax.set_ylim(all_y.min() - ypad, all_y.max() + ypad)\n",
        "\n",
        "    ax.set_xlabel('Standard Deviation')\n",
        "    ax.set_ylabel('Expected Return')\n",
        "    ax.set_title(title)\n",
        "    ax.legend(frameon=True, loc='best')\n",
        "    ax.grid(True, linestyle=':', alpha=0.7)\n",
        "\n",
        "    if use_letters and letter_map:\n",
        "        _add_letter_legend(ax, letter_map, title=\"Letter → Portfolio\", loc=letter_legend_loc)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    return fig, ax, letter_map\n",
        "\n",
        "def plot_is_os_lw_frontier(\n",
        "    pfslw: pd.DataFrame, means: pd.Series, lw_covmat: pd.DataFrame, rfrate: float, title: str,\n",
        "    color_is='#1f77b4', color_os='#ff7f0e', cal_color='#2ca02c', frontier_color='#17becf',\n",
        "    use_letters: bool = True, letter_legend_loc: str = \"lower right\"\n",
        "):\n",
        "    for col in ['Standard Deviation', 'Expected Return']:\n",
        "        pfslw[col] = pd.to_numeric(pfslw[col], errors='coerce')\n",
        "\n",
        "    idx_norm = pfslw.index.astype(str).str.lower().str.strip()\n",
        "    is_mask = idx_norm.str.startswith('is ')\n",
        "    os_mask = idx_norm.str.startswith('os ')\n",
        "    letter_map = _letter_map_from_index(pfslw.index.tolist()) if use_letters else {}\n",
        "\n",
        "    sns.set(style='whitegrid')\n",
        "    fig, ax = plt.subplots(figsize=(8.5,6.5), dpi=160)\n",
        "\n",
        "    if pfslw.loc[is_mask].size:\n",
        "        sub = pfslw.loc[is_mask, ['Standard Deviation','Expected Return']].dropna()\n",
        "        ax.scatter(sub['Standard Deviation'], sub['Expected Return'],\n",
        "                   c=color_is, s=110, label='IS', zorder=4,\n",
        "                   edgecolors=\"white\", linewidths=1.2)\n",
        "        _annotate_static(ax, pfslw, is_mask, color_is, letter_map, dy_pixels=12)\n",
        "\n",
        "    if pfslw.loc[os_mask].size:\n",
        "        sub = pfslw.loc[os_mask, ['Standard Deviation','Expected Return']].dropna()\n",
        "        ax.scatter(sub['Standard Deviation'], sub['Expected Return'],\n",
        "                   c=color_os, s=130, marker='^', label='OOS', zorder=4,\n",
        "                   edgecolors=\"white\", linewidths=1.2)\n",
        "        _annotate_static(ax, pfslw, os_mask, color_os, letter_map, dy_pixels=-14)\n",
        "\n",
        "    # CAL via \"IS LW QP ORP\"\n",
        "    tag = 'is lw qp orp'\n",
        "    poss = idx_norm[idx_norm.str.contains(tag)].tolist()\n",
        "    if poss:\n",
        "        label = pfslw.index[idx_norm == poss[0]][0]\n",
        "        sd = float(pfslw.loc[label, 'Standard Deviation']); er = float(pfslw.loc[label, 'Expected Return'])\n",
        "        if np.isfinite(sd) and np.isfinite(er):\n",
        "            slope = (er - rfrate)/max(sd, 1e-12)\n",
        "            x_max = max(float(pfslw['Standard Deviation'].max())*1.25, sd*1.35)\n",
        "            xs = np.linspace(0, x_max, 400); ys = rfrate + slope*xs\n",
        "            ax.plot(xs, ys, color=cal_color, linestyle='--', linewidth=2.4,\n",
        "                    label=f'CAL via {label}', zorder=2.5)\n",
        "            ax.scatter([0],[rfrate], color=cal_color, edgecolor='k', zorder=5)\n",
        "\n",
        "    # Efficient frontier (LW; keep full line)\n",
        "    sig, mu = efficient_frontier_unconstrained(means, lw_covmat, n=400)\n",
        "    if sig.size and mu.size:\n",
        "        ax.plot(sig, mu, color=frontier_color, linewidth=2.8,\n",
        "                label='Efficient Frontier (IS, LW)', zorder=2)\n",
        "\n",
        "    all_x = np.concatenate([pfslw['Standard Deviation'].dropna().values, sig])\n",
        "    all_y = np.concatenate([pfslw['Expected Return'].dropna().values,     mu ])\n",
        "    xpad = (all_x.max() - all_x.min()) * 0.08 if all_x.size else 0.01\n",
        "    ypad = (all_y.max() - all_y.min()) * 0.08 if all_y.size else 0.01\n",
        "    ax.set_xlim(all_x.min() - xpad, all_x.max() + xpad)\n",
        "    ax.set_ylim(all_y.min() - ypad, all_y.max() + ypad)\n",
        "\n",
        "    ax.set_xlabel('Standard Deviation')\n",
        "    ax.set_ylabel('Expected Return')\n",
        "    ax.set_title(title)\n",
        "    ax.legend(frameon=True, loc='best')\n",
        "    ax.grid(True, linestyle=':', alpha=0.7)\n",
        "\n",
        "    if use_letters and letter_map:\n",
        "        _add_letter_legend(ax, letter_map, title=\"Letter → Portfolio\", loc=letter_legend_loc)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    return fig, ax, letter_map\n",
        "\n",
        "# ---------- SAVE ----------\n",
        "# %matplotlib inline  # (Jupyter)\n",
        "\n",
        "# 1) SAMPLE Σ\n",
        "fig, ax, letter_map_sample = plot_is_os_frontier(\n",
        "    pfs, means, covmat, rfrate,\n",
        "    title=\"Sample Covariance: IS and OOS Points\",\n",
        "    use_letters=True\n",
        ")\n",
        "# plt.show()\n",
        "fig.savefig(\"outputs/samplecovmat_graph.png\", dpi=180, bbox_inches=\"tight\"); plt.close(fig)\n",
        "\n",
        "# 2) LEDOIT–WOLF Σ\n",
        "fig2, ax2, letter_map_lw = plot_is_os_lw_frontier(\n",
        "    pfslw, means, lw_covmat, rfrate,\n",
        "    title=\"Ledoit–Wolf Covariance: IS and OOS Points\",\n",
        "    use_letters=True\n",
        ")\n",
        "# plt.show()\n",
        "fig2.savefig(\"outputs/lwcovmat_graph.png\", dpi=180, bbox_inches=\"tight\"); plt.close(fig2)\n",
        "plt.close(fig)\n",
        "plt.close(fig2)"
      ],
      "metadata": {
        "id": "iXiaLPT8K5kW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b995a804-c0d9-4b46-f79c-012456398600"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-250749323.py:11: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  a = float(one.T @ P @ one); b = float(one.T @ P @ mu); c = float(mu.T @ P @ mu)\n",
            "/tmp/ipython-input-250749323.py:11: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  a = float(one.T @ P @ one); b = float(one.T @ P @ mu); c = float(mu.T @ P @ mu)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ Excel Report Builder (clean, percent-safe) ============\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from openpyxl import Workbook, load_workbook\n",
        "from openpyxl.utils import get_column_letter\n",
        "from openpyxl.styles import Font, PatternFill\n",
        "from openpyxl.drawing.image import Image as XLImage\n",
        "\n",
        "# ---------- helpers ----------\n",
        "def _autosize(ws, max_width=48):\n",
        "    for col in ws.columns:\n",
        "        col_letter = get_column_letter(col[0].column)\n",
        "        width = 0\n",
        "        for cell in col:\n",
        "            txt = \"\" if cell.value is None else str(cell.value)\n",
        "            width = max(width, len(txt))\n",
        "        ws.column_dimensions[col_letter].width = min(max(10, width + 2), max_width)\n",
        "\n",
        "def _write_df(ws, df: pd.DataFrame, start_row=1, start_col=1, header=True, index_as_col=True):\n",
        "    r, c = start_row, start_col\n",
        "    if header:\n",
        "        if index_as_col:\n",
        "            ws.cell(r, c).value = \"\"\n",
        "            ws.cell(r, c).font = Font(bold=True)\n",
        "            for j, col in enumerate(df.columns, start=c+1):\n",
        "                ws.cell(r, j).value = str(col)\n",
        "                ws.cell(r, j).font = Font(bold=True)\n",
        "        else:\n",
        "            for j, col in enumerate(df.columns, start=c):\n",
        "                ws.cell(r, j).value = str(col)\n",
        "                ws.cell(r, j).font = Font(bold=True)\n",
        "        r += 1\n",
        "    for idx, row in df.iterrows():\n",
        "        if index_as_col:\n",
        "            ws.cell(r, c).value = str(idx)\n",
        "            ws.cell(r, c).font = Font(bold=True)\n",
        "            for j, col in enumerate(df.columns, start=c+1):\n",
        "                val = row[col]\n",
        "                ws.cell(r, j).value = float(val) if pd.api.types.is_number(val) else (None if pd.isna(val) else str(val))\n",
        "        else:\n",
        "            for j, col in enumerate(df.columns, start=c):\n",
        "                val = row[col]\n",
        "                ws.cell(r, j).value = float(val) if pd.api.types.is_number(val) else (None if pd.isna(val) else str(val))\n",
        "        r += 1\n",
        "    return r\n",
        "\n",
        "def _format_percent(ws, header_row=1, include_cols=None, exclude_cols=None):\n",
        "    # exact matches are case-insensitive\n",
        "    headers = {}\n",
        "    for cell in ws[header_row]:\n",
        "        if cell.value:\n",
        "            headers[cell.column] = str(cell.value).strip()\n",
        "    norm = lambda s: s.lower().strip()\n",
        "    include = {norm(c) for c in include_cols} if include_cols else set()\n",
        "    exclude = {norm(c) for c in exclude_cols} if exclude_cols else set()\n",
        "    keywords = [\"expected return\", \"standard deviation\", \"return\", \"ret\", \"stdev\", \"std dev\", \"vol\", \"volatility\"]\n",
        "    for ci, name in headers.items():\n",
        "        n = norm(name)\n",
        "        if n in exclude:\n",
        "            continue\n",
        "        use_pct = (n in include) if include else (any(k in n for k in keywords) or n.endswith(\"(ann.)\") or n.endswith(\"_a\") or n.endswith(\"_m\"))\n",
        "        if not use_pct:\n",
        "            continue\n",
        "        col_letter = get_column_letter(ci)\n",
        "        for cell in ws[col_letter][header_row:]:\n",
        "            if isinstance(cell.value, (int, float)):\n",
        "                cell.number_format = \"0.00%\"\n",
        "\n",
        "def _format_ratio(ws, header_row=1):\n",
        "    headers = {}\n",
        "    for cell in ws[header_row]:\n",
        "        if cell.value:\n",
        "            headers[cell.column] = str(cell.value).lower()\n",
        "    for ci, name in headers.items():\n",
        "        if \"sharpe\" in name or \"ratio\" in name:\n",
        "            col_letter = get_column_letter(ci)\n",
        "            for cell in ws[get_column_letter(ci)][header_row:]:\n",
        "                if isinstance(cell.value, (int, float)):\n",
        "                    cell.number_format = \"0.000\"\n",
        "\n",
        "def _freeze(ws, row=2, col=2):\n",
        "    ws.freeze_panes = ws.cell(row=row, column=col)\n",
        "\n",
        "def _stripe(ws, header_row=1):\n",
        "    fill = PatternFill(start_color=\"FFF5F5F5\", end_color=\"FFF5F5F5\", fill_type=\"solid\")\n",
        "    on = False\n",
        "    for r in range(header_row+1, ws.max_row+1):\n",
        "        on = not on\n",
        "        if on:\n",
        "            for c in range(1, ws.max_column+1):\n",
        "                ws.cell(r, c).fill = fill\n",
        "\n",
        "# ---------- main builder ----------\n",
        "def build_excel_report(\n",
        "    path_xlsx: str,\n",
        "    inputs: dict,\n",
        "    pfs: pd.DataFrame,      # sample-cov metrics (rows like IS ..., OS ...)\n",
        "    pfslw: pd.DataFrame,    # LW metrics (rows like IS ..., OS ...)\n",
        "    is_returns: pd.DataFrame,\n",
        "    os_returns: pd.DataFrame,\n",
        "    is_covrep: pd.DataFrame = None,\n",
        "    os_covrep: pd.DataFrame = None,\n",
        "    image_paths: list = None\n",
        "):\n",
        "    wb = Workbook()\n",
        "\n",
        "    # Inputs\n",
        "    ws = wb.active\n",
        "    ws.title = \"Inputs\"\n",
        "    ws[\"A1\"].value = \"Parameter\"; ws[\"A1\"].font = Font(bold=True)\n",
        "    ws[\"B1\"].value = \"Value\";     ws[\"B1\"].font = Font(bold=True)\n",
        "    r = 2\n",
        "    for k, v in inputs.items():\n",
        "        ws[f\"A{r}\"] = str(k)\n",
        "        ws[f\"B{r}\"] = str(v)\n",
        "        r += 1\n",
        "    _autosize(ws); _freeze(ws)\n",
        "\n",
        "    # IS Metrics (sample Σ)\n",
        "    ws_is = wb.create_sheet(\"IS Metrics\")\n",
        "    _write_df(ws_is, pfs.loc[pfs.index.str.startswith(\"IS\")], header=True, index_as_col=True)\n",
        "    _format_percent(ws_is, header_row=1,\n",
        "                    include_cols=[\"Expected Return\",\"Standard Deviation\",\"Expected Return (Ann.)\",\"Standard Deviation (Ann.)\"],\n",
        "                    exclude_cols=[\"Sharpe Ratio (Monthly)\",\"Sharpe Ratio (Annualized)\"])\n",
        "    _format_ratio(ws_is, header_row=1)\n",
        "    _stripe(ws_is); _autosize(ws_is); _freeze(ws_is)\n",
        "\n",
        "    # OS Metrics (sample Σ)\n",
        "    ws_os = wb.create_sheet(\"OS Metrics\")\n",
        "    _write_df(ws_os, pfs.loc[pfs.index.str.startswith(\"OS\")], header=True, index_as_col=True)\n",
        "    _format_percent(ws_os, header_row=1,\n",
        "                    include_cols=[\"Expected Return\",\"Standard Deviation\",\"Expected Return (Ann.)\",\"Standard Deviation (Ann.)\"],\n",
        "                    exclude_cols=[\"Sharpe Ratio (Monthly)\",\"Sharpe Ratio (Annualized)\"])\n",
        "    _format_ratio(ws_os, header_row=1)\n",
        "    _stripe(ws_os); _autosize(ws_os); _freeze(ws_os)\n",
        "\n",
        "    # IS–LW Metrics\n",
        "    ws_is_lw = wb.create_sheet(\"IS–LW Metrics\")\n",
        "    _write_df(ws_is_lw, pfslw.loc[pfslw.index.str.startswith(\"IS\")], header=True, index_as_col=True)\n",
        "    _format_percent(ws_is_lw, header_row=1,\n",
        "                    include_cols=[\"Expected Return\",\"Standard Deviation\",\"Expected Return (Ann.)\",\"Standard Deviation (Ann.)\"],\n",
        "                    exclude_cols=[\"Sharpe Ratio (Monthly)\",\"Sharpe Ratio (Annualized)\"])\n",
        "    _format_ratio(ws_is_lw, header_row=1)\n",
        "    _stripe(ws_is_lw); _autosize(ws_is_lw); _freeze(ws_is_lw)\n",
        "\n",
        "    # OS–LW Metrics\n",
        "    ws_os_lw = wb.create_sheet(\"OS–LW Metrics\")\n",
        "    _write_df(ws_os_lw, pfslw.loc[pfslw.index.str.startswith(\"OS\")], header=True, index_as_col=True)\n",
        "    _format_percent(ws_os_lw, header_row=1,\n",
        "                    include_cols=[\"Expected Return\",\"Standard Deviation\",\"Expected Return (Ann.)\",\"Standard Deviation (Ann.)\"],\n",
        "                    exclude_cols=[\"Sharpe Ratio (Monthly)\",\"Sharpe Ratio (Annualized)\"])\n",
        "    _format_ratio(ws_os_lw, header_row=1)\n",
        "    _stripe(ws_os_lw); _autosize(ws_os_lw); _freeze(ws_os_lw)\n",
        "\n",
        "    # Monthly Returns (IS)\n",
        "    ws_ris = wb.create_sheet(\"Monthly Returns (IS)\")\n",
        "    df_is = is_returns.copy()\n",
        "    df_is.index = pd.to_datetime(df_is.index)\n",
        "    table = df_is.sort_index().reset_index()\n",
        "    table.rename(columns={table.columns[0]: \"Date\"}, inplace=True)\n",
        "    table[\"Date\"] = pd.to_datetime(table[\"Date\"]).dt.strftime(\"%Y-%m\")\n",
        "    _write_df(ws_ris, table, header=True, index_as_col=False)\n",
        "    for col in range(2, ws_ris.max_column+1):\n",
        "        for cell in ws_ris[get_column_letter(col)][1:]:\n",
        "            if isinstance(cell.value, (int, float)):\n",
        "                cell.number_format = \"0.00%\"\n",
        "    _autosize(ws_ris); _freeze(ws_ris)\n",
        "\n",
        "    # Monthly Returns (OS)\n",
        "    ws_ros = wb.create_sheet(\"Monthly Returns (OS)\")\n",
        "    df_os = os_returns.copy()\n",
        "    df_os.index = pd.to_datetime(df_os.index)\n",
        "    table = df_os.sort_index().reset_index()\n",
        "    table.rename(columns={table.columns[0]: \"Date\"}, inplace=True)\n",
        "    table[\"Date\"] = pd.to_datetime(table[\"Date\"]).dt.strftime(\"%Y-%m\")\n",
        "    _write_df(ws_ros, table, header=True, index_as_col=False)\n",
        "    for col in range(2, ws_ros.max_column+1):\n",
        "        for cell in ws_ros[get_column_letter(col)][1:]:\n",
        "            if isinstance(cell.value, (int, float)):\n",
        "                cell.number_format = \"0.00%\"\n",
        "    _autosize(ws_ros); _freeze(ws_ros)\n",
        "\n",
        "    # Coverage (optional)\n",
        "    if is_covrep is not None:\n",
        "        ws_cov_is = wb.create_sheet(\"Coverage (IS)\")\n",
        "        _write_df(ws_cov_is, is_covrep, header=True, index_as_col=True)\n",
        "        _format_percent(ws_cov_is, header_row=1)  # if your frac column is 0..1\n",
        "        _stripe(ws_cov_is); _autosize(ws_cov_is); _freeze(ws_cov_is)\n",
        "\n",
        "    if os_covrep is not None:\n",
        "        ws_cov_os = wb.create_sheet(\"Coverage (OS)\")\n",
        "        _write_df(ws_cov_os, os_covrep, header=True, index_as_col=True)\n",
        "        _format_percent(ws_cov_os, header_row=1)\n",
        "        _stripe(ws_cov_os); _autosize(ws_cov_os); _freeze(ws_cov_os)\n",
        "\n",
        "    # Plots\n",
        "    ws_plots = wb.create_sheet(\"Plots\")\n",
        "    cur_row = 1\n",
        "    if image_paths:\n",
        "        for p in image_paths:\n",
        "            try:\n",
        "                img = XLImage(p)\n",
        "                img.anchor = f\"A{cur_row}\"\n",
        "                ws_plots.add_image(img)\n",
        "                advance = max(int(img.height / 18) + 2, 20)\n",
        "                cur_row += advance\n",
        "            except Exception as e:\n",
        "                ws_plots.cell(cur_row, 1).value = f\"Could not embed {p}: {e}\"\n",
        "                cur_row += 2\n",
        "    _autosize(ws_plots)\n",
        "\n",
        "    Path(path_xlsx).parent.mkdir(parents=True, exist_ok=True)\n",
        "    wb.save(path_xlsx)\n",
        "    print(f\"[Excel] Saved {path_xlsx}\")\n",
        "    return path_xlsx\n"
      ],
      "metadata": {
        "id": "SOmr9Jc6qMrR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- Make sure the images you embed are the ones you saved ----\n",
        "# You saved to: outputs/samplecovmat_graph.png and outputs/lwcovmat_graph.png\n",
        "# so use those here (and feel free to add more later).\n",
        "from pathlib import Path\n",
        "outdir = Path(\"outputs\")\n",
        "outdir.mkdir(exist_ok=True)\n",
        "\n",
        "image_paths = [\n",
        "    str(outdir / \"samplecovmat_graph.png\"),\n",
        "    str(outdir / \"lwcovmat_graph.png\"),\n",
        "]\n",
        "\n",
        "# ---- Optional but nice: build weights tables for Excel ----\n",
        "# Sample-Σ weights\n",
        "weights_sample = (\n",
        "    pd.DataFrame({\n",
        "        \"Original\":      weights.reindex(ret_matrix.columns),\n",
        "        \"GMV\":           gmv.reindex(ret_matrix.columns),\n",
        "        \"Efficient\":     efport.reindex(ret_matrix.columns),\n",
        "        \"ORP\":           orp.reindex(ret_matrix.columns),\n",
        "        \"QP ORP\":        qporp.reindex(ret_matrix.columns)\n",
        "    })\n",
        "    .rename_axis(\"Ticker\")\n",
        ")\n",
        "\n",
        "# Ledoit–Wolf-Σ weights\n",
        "weights_lw = (\n",
        "    pd.DataFrame({\n",
        "        \"LW GMV\":        lwgmv.reindex(ret_matrix.columns),\n",
        "        \"LW Efficient\":  lwefport.reindex(ret_matrix.columns),\n",
        "        \"LW ORP\":        lworp.reindex(ret_matrix.columns),\n",
        "        \"LW QP ORP\":     lwqporp.reindex(ret_matrix.columns)\n",
        "    })\n",
        "    .rename_axis(\"Ticker\")\n",
        ")\n",
        "\n",
        "# Differences (LW minus Sample where comparable)\n",
        "# Only compare columns with the same semantics\n",
        "overlap_cols = [\"GMV\", \"Efficient\", \"ORP\", \"QP ORP\"]\n",
        "diff_cols = {f\"Δ {c} (LW - Sample)\": (weights_lw.get(\"LW \"+c) - weights_sample.get(c))\n",
        "             for c in overlap_cols if (\"LW \"+c) in weights_lw.columns and c in weights_sample.columns}\n",
        "weights_diff = pd.DataFrame(diff_cols).rename_axis(\"Ticker\")\n",
        "\n",
        "# ---- Annualize the metrics (optional lines to make Excel nicer to read) ----\n",
        "def _annualize(df):\n",
        "    df = df.copy()\n",
        "    if \"Expected Return\" in df.columns:\n",
        "        df[\"Expected Return (Ann.)\"] = df[\"Expected Return\"] * 12\n",
        "    if \"Standard Deviation\" in df.columns:\n",
        "        df[\"Standard Deviation (Ann.)\"] = df[\"Standard Deviation\"] * (12 ** 0.5)\n",
        "    return df\n",
        "\n",
        "pfs_out   = _annualize(pfs)\n",
        "pfslw_out = _annualize(pfslw)\n",
        "\n",
        "# ---- Inputs panel (shown on first sheet) ----\n",
        "inputs = {\n",
        "    \"Tickers\": \", \".join(list(ret_matrix.columns)),\n",
        "    \"IS Range\": f\"{ret_matrix.index.min():%Y-%m} – {ret_matrix.index.max():%Y-%m}\",\n",
        "    \"OOS Range\": f\"{os_ret_matrix.index.min():%Y-%m} – {os_ret_matrix.index.max():%Y-%m}\",\n",
        "    \"Risk-free (monthly)\": f\"{rfrate:.6f}\",\n",
        "    \"Constraints\": \"Unconstrained (weights sum to 1) unless stated for QP\",\n",
        "    \"QP bounds\": \"[-1, 1] in this run (edit in code)\",\n",
        "}\n",
        "\n",
        "# ---- Build the Excel workbook (adds metrics, returns, and plots you specify) ----\n",
        "# NOTE: build_excel_report is already defined in your file.\n",
        "# We’ll extend it here by writing the extra weight sheets after it saves.\n",
        "xlsx_path = outdir / \"Project2_Report.xlsx\"\n",
        "\n",
        "build_excel_report(\n",
        "    path_xlsx=str(xlsx_path),\n",
        "    inputs=inputs,\n",
        "    pfs=pfs_out,          # Sample-Σ metrics (IS* and OOS*)\n",
        "    pfslw=pfslw_out,      # LW-Σ metrics (IS* and OOS*)\n",
        "    is_returns=ret_matrix,\n",
        "    os_returns=os_ret_matrix,\n",
        "    is_covrep=None,\n",
        "    os_covrep=None,\n",
        "    image_paths=image_paths\n",
        ")\n",
        "\n",
        "# ---- Append weight sheets to the same workbook ----\n",
        "from openpyxl import load_workbook\n",
        "wb = load_workbook(str(xlsx_path))\n",
        "\n",
        "def _append_df_sheet(wb, title, df, pct_cols=None):\n",
        "    ws = wb.create_sheet(title)\n",
        "    # write header\n",
        "    ws.cell(1,1).value = \"Ticker\"; ws.cell(1,1).font = Font(bold=True)\n",
        "    for j, c in enumerate(df.columns, start=2):\n",
        "        ws.cell(1, j).value = c; ws.cell(1, j).font = Font(bold=True)\n",
        "    # write rows\n",
        "    r = 2\n",
        "    for idx, row in df.iterrows():\n",
        "        ws.cell(r,1).value = str(idx)\n",
        "        for j, c in enumerate(df.columns, start=2):\n",
        "            val = row[c]\n",
        "            ws.cell(r, j).value = float(val) if pd.notna(val) else None\n",
        "        r += 1\n",
        "    # formatting\n",
        "    # treat all weight columns as percents\n",
        "    from openpyxl.styles import numbers\n",
        "    for col in range(2, 2 + len(df.columns)):\n",
        "        for rr in range(2, r):\n",
        "            cell = ws.cell(rr, col)\n",
        "            if isinstance(cell.value, (int, float)):\n",
        "                cell.number_format = \"0.00%\"\n",
        "    # tidy\n",
        "    _autosize(ws); _freeze(ws, row=2, col=2); _stripe(ws)\n",
        "\n",
        "_append_df_sheet(wb, \"Weights (Sample Σ)\", weights_sample)\n",
        "_append_df_sheet(wb, \"Weights (LW Σ)\",     weights_lw)\n",
        "_append_df_sheet(wb, \"Weights Δ (LW-Sample)\", weights_diff)\n",
        "\n",
        "wb.save(str(xlsx_path))\n",
        "print(f\"[Excel] Appended weight sheets → {xlsx_path}\")\n"
      ],
      "metadata": {
        "id": "szOMM59XGXpU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4554706-a334-4bf2-baae-9c603ba1f0b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Excel] Saved outputs/Project2_Report.xlsx\n",
            "[Excel] Appended weight sheets → outputs/Project2_Report.xlsx\n"
          ]
        }
      ]
    }
  ]
}